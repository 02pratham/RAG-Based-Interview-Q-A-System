RAG-Based Interview Q&A System
This project is a Retrieval-Augmented Generation (RAG) system designed to conduct and evaluate technical interviews. It uses a candidate's CV, a job description (JD), and a set of reference documents to generate relevant questions and then evaluates the candidate's answers. The system is built with a focus on contextual understanding, ensuring that questions and answers are grounded in the provided documents.

Key Features
Contextual Question Generation: Generates interview questions based on a candidate's CV and a job description.

Reference Document Augmentation: Uses a RAG pipeline to ground questions in a set of reference documents, ensuring factual accuracy.

Automated Answer Evaluation: Integrates with the DeepEval framework to assess the quality of a candidate's answers against the generated "golden" answers. Metrics include:

Answer Relevancy

Contextual Precision

Contextual Recall

Faithfulness

Modular Architecture: The codebase is organized into distinct modules for document loading, retrieval, LLM interaction, and evaluation, making it easy to extend and maintain.

Groq API Integration: Leverages the speed of the Groq API for lightning-fast LLM responses.

Project Structure
.
├── src/
│   ├── adapters/
│   │   ├── groq_adapter.py    # DeepEval adapter for Groq LLM
│   ├── utils/
│   │   └── logging.py         # Utility for colored terminal logging
│   ├── config.py              # Configuration for API keys and RAG parameters
│   ├── document_loader.py     # Handles loading and parsing various document types (PDF, RTF)
│   ├── evaluation.py          # Logic for running DeepEval metrics
│   ├── interview.py           # Core logic for interview session management
│   ├── llm_engine.py          # Handles all interactions with the LLM
│   └── retrieval.py           # RAG implementation with Sentence Window Retrieval
├── run_interview.py           # Main script to run an interactive interview session
├── run_evaluation.py          # Main script to evaluate a completed interview
├── interview_data.json        # Output file containing interview results (generated by run_interview.py)
├── evaluation_results.json    # Output file containing evaluation scores (generated by run_evaluation.py)
└── .env.example               # Template for environment variables

Getting Started
Prerequisites
Python 3.8+

A Groq API key

Installation
Clone the repository:

git clone [https://github.com/02pratham/RAG-Based-Interview-Q-A-System.git](https://github.com/02pratham/RAG-Based-Interview-Q-A-System.git)
cd RAG-Based-Interview-Q-A-System

Create a virtual environment and install dependencies:

python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
pip install -r requirements.txt

Set up your environment variables:

Rename .env.example to .env.

Add your Groq API key to the .env file.

GROQ_API_KEY="your_groq_api_key_here"

Usage
1. Running an Interview Session
To start a new interview, you must provide a candidate's CV, a job description, and at least one reference document.

python run_interview.py \
    --cv path/to/your/cv.pdf \
    --jd path/to/your/job_description.pdf \
    --refs path/to/reference_doc1.pdf path/to/reference_doc2.pdf

The script will generate questions one by one, and you will be prompted to provide an answer for each. All interview data, including questions, your answers, model-generated answers, and relevant context, will be saved to interview_data.json.

Non-Interactive Mode:
If you have a file with pre-written answers, you can run the interview in non-interactive mode. Each answer should be on a new line.

python run_interview.py \
    --cv path/to/your/cv.pdf \
    --jd path/to/your/job_description.pdf \
    --refs path/to/reference_doc1.pdf \
    --non_interactive_answers path/to/answers.txt

2. Evaluating Interview Answers
After running an interview, you can evaluate the results using the run_evaluation.py script.

python run_evaluation.py \
    --data interview_data.json \
    --threshold 0.7

This script will read the interview_data.json file and use DeepEval to compute metrics for each question. The results, including scores and reasons, will be saved to evaluation_results.json. The --threshold argument (optional, default is 0.7) sets the minimum acceptable score for each metric.

Configuration
You can customize the application by editing the .env file.

Variable

Description

Default Value

GROQ_API_KEY

Your Groq API key.

""

GROQ_MODEL

The Groq LLM model to use.

"llama-3.3-70b-versatile"

EMBEDDING_MODEL

The HuggingFace embedding model for retrieval.

"BAAI/bge-small-en-v1.5"

SENTENCE_WINDOW_SIZE

The number of sentences to include in the context window.

5

SIMILARITY_TOP_K

Number of most similar documents to retrieve.

6

RERANK_TOP_N

Number of top documents to re-rank for better accuracy.

2

INDEX_DIR

Directory to store the vector index.

".sentence_index"

